<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="This work introduces TokenFlow, a novel unified image tokenizer that bridges the long-standing gap between multimodal understanding and generation.">
  <meta property="og:title" content="TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation"/>
  <meta property="og:description" content="This work introduces TokenFlow, a novel unified image tokenizer that bridges the long-standing gap between multimodal understanding and generation."/>
  <meta property="og:url" content="https://byteflow-ai.github.io/TokenFlow/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/teasor_cmb.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation">
  <meta name="twitter:description" content="This work introduces TokenFlow, a novel unified image tokenizer that bridges the long-standing gap between multimodal understanding and generation.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/teasor_cmb.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="unified modeling, VLM, text-to-image, next-scale prediction, VAR">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>TokenFlow</title>
<!--  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">-->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://github.com/QuLiao1117" target="_blank">Liao Qu</a><sup>*</sup>,</span>
              <span class="author-block">
                <a href="mailto:muxizju@gmail.com" target="_blank">Huichao Zhang</a><sup>*</sup>,</span>
              <span class="author-block">
                <a href="https://yolomax.com/" target="_blank">Yiheng Liu</a><sup></sup>,</span>
               <span class="author-block">Xu Wang<sup>&#8224</sup>,</span>
               <span class="author-block">
                <a href="https://enjoyyi.github.io/" target="_blank">Yi Jiang</a><sup></sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=uRCc-McAAAAJ&hl=zh-TW" target="_blank">Yiming Gao</a><sup></sup>,</span>
              <span class="author-block">
                <a href="https://github.com/xiaohu2015" target="_blank">Hu Ye</a><sup></sup>,</span>
              <br>

              <span class="author-block">Daniel K. Du<sup></sup>,</span>
              <span class="author-block">Zehuan Yuan<sup></sup>,</span>
              <span class="author-block">Xinglong Wu<sup></sup>

            </div>

                 <div class="is-size-5 publication-authors">
                    <span class="author-block"> <sup></sup>ByteDance</span>
                   <!-- <br>Conferance name and year -->
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2412.03069" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

<!--                    &lt;!&ndash; Supplementary PDF link &ndash;&gt;-->
<!--                    <span class="link-block">-->
<!--                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"-->
<!--                      class="external-link button is-normal is-rounded is-dark">-->
<!--                      <span class="icon">-->
<!--                        <i class="fas fa-file-pdf"></i>-->
<!--                      </span>-->
<!--                      <span>Supplementary</span>-->
<!--                    </a>-->
<!--                  </span>-->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/ByteFlow-AI/TokenFlow" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://huggingface.co/ByteFlow-AI" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <span>&#129303;</span>
                  </span>
                  <span>Models</span>
                </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="TODO" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container"  align="center">
    <div class="hero-body">
        <img src="static/images/teasor_cmb.png" alt="MY ALT TEXT" style="width: 80%;"/>
        <!-- <h2 class="subtitle has-text-centered">
          Overview of the proposed layer-wise calibration procedure before fine-tuning.
        </h2> -->
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present TokenFlow, a novel unified image tokenizer that bridges the long-standing gap between multimodal under-standing and generation.
            Prior research attempt to employ a single reconstruction-targeted Vector Quantization (VQ)encoder for unifying these two tasks.
            We observe that understanding and generation require fundamentally different granularities of visual information.
            This leads to a crit-ical trade-off, particularly compromising performance in multimodal understanding tasks.
            TokenFlow addresses this challenge through an innovative dual-codebook architecture that decouples semantic and pixel-level feature learn-ing while maintaining their alignment via a shared map-ping mechanism.
            This design enables direct access to both high-level semantic representations crucial for understand-ing tasks and fine-grained visual features essential for generation through shared indices.
            Our extensive experiments demonstrate TokenFlow’s superiority across multiple dimensions. Leveraging TokenFlow,
            we demonstrate for the first time that discrete visual input can surpass LLaVA-1.5 13B in understanding performance, achieving a 7.2% average improvement.
            For image reconstruction, we achieve a strong FID score of 0.63 at 384×384 resolution. Moreover, TokenFlow establishes state-of-the-art performance in autoregressive image generation with a GenEval score of 0.55 at 256×256 resolution, achieving comparable results to SDXL.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

  <!-- Paper method -->
<section class="section hero ">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <img src="static/images/method.png" alt="MY ALT TEXT" style="width: 100%;"/>

          <p>
            <b>Shared Mapping is the key to bridge generation and understanding ability in one tokenizer for AR model.</b>
            We incorporate dual encoders and codebooks with a shared mapping, enabling the joint optimization of high-level
            semantics and low-level pixel details. For a given input image, distances are calculated from the pixel-level and semantic-level codebooks,
            respectively, with the final codebook index and features determined by minimizing the weighted sum distance.
            The resulting quantized features are independently decoded for both semantic alignment and image reconstruction training,
            and then concatenated to provide a unified representation for downstream tasks in understanding and generation.
          </p>
        </div>

        <div class="content has-text-justified">
          <img src="static/images/cluster.png" alt="MY ALT TEXT" style="width: 100%;"/>

          <p>
            <b>TokenFlow can combine both semantic and low-level similarity (e.g. birds with different background can be mapped into two different index) and exhibits significantly smoother distribution compared to others.</b>
          </p>
        </div>

      </div>
    </div>
  </div>
</section>
<!-- End paper method -->


  <!-- Teaser video-->
<section class="hero teaser">
  <div class="container" >
      <h2 class="subtitle has-text-centered">
       <font size="20"> Multimodal Understanding</font>
</h2>
    <div class="container"  align="center">
        <img src="static/images/vlm.png" alt="MY ALT TEXT"  align="center" style=" max-width: 80%" />
        <h2 class="subtitle has-text-centered">
          TokenFlow-XL surpasses Llava-1.5 13B by 7% on average.
        </h2>
    </div>
  </div>
</section>
<br><br>

  <section class="hero teaser">
  <div class="container" >
      <h2 class="subtitle has-text-centered">
       <font size="20"> Text to Image Generation</font>
</h2>
    <div class="container" align="center">
        <img src="static/images/t2i_metric.png" alt="MY ALT TEXT" width="80%" />
      <br><br>
        <img src="static/images/teasor.png" alt="MY ALT TEXT" width="80%" />

    </div>
  </div>
</section>
</section>
<br><br>


  <!-- End image carousel -->

<!--&lt;!&ndash; Youtube video &ndash;&gt;-->
<!--<section class="hero is-small is-light">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      &lt;!&ndash; Paper video. &ndash;&gt;-->
<!--      <h2 class="title is-3">Video Presentation</h2>-->
<!--      <div class="columns is-centered has-text-centered">-->
<!--        <div class="column is-four-fifths">-->
<!--          -->
<!--          <div class="publication-video">-->
<!--            &lt;!&ndash; Youtube embed code here &ndash;&gt;-->
<!--            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>-->
<!--          </div>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->
<!--&lt;!&ndash; End youtube video &ndash;&gt;-->


<!--&lt;!&ndash; Video carousel &ndash;&gt;-->
<!--<section class="hero is-small">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      <h2 class="title is-3">Another Carousel</h2>-->
<!--      <div id="results-carousel" class="carousel results-carousel">-->
<!--        <div class="item item-video1">-->
<!--          <video poster="" id="video1" autoplay controls muted loop height="100%">-->
<!--            &lt;!&ndash; Your video file here &ndash;&gt;-->
<!--            <source src="static/videos/carousel1.mp4"-->
<!--            type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-video2">-->
<!--          <video poster="" id="video2" autoplay controls muted loop height="100%">-->
<!--            &lt;!&ndash; Your video file here &ndash;&gt;-->
<!--            <source src="static/videos/carousel2.mp4"-->
<!--            type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-video3">-->
<!--          <video poster="" id="video3" autoplay controls muted loop height="100%">\-->
<!--            &lt;!&ndash; Your video file here &ndash;&gt;-->
<!--            <source src="static/videos/carousel3.mp4"-->
<!--            type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->
<!--&lt;!&ndash; End video carousel &ndash;&gt;-->



<!--&lt;!&ndash; Paper poster &ndash;&gt;-->
<!--<section class="hero is-small is-light">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      <h2 class="title">Poster</h2>-->

<!--      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">-->
<!--          </iframe>-->
<!--        -->
<!--      </div>-->
<!--    </div>-->
<!--  </section>-->
<!--&lt;!&ndash;End paper poster &ndash;&gt;-->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
      @article{qu2024tokenflow,
        title={TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation},
        author={Qu, Liao and Zhang, Huichao and Liu, Yiheng and Wang, Xu and Jiang, Yi and Gao, Yiming and Ye, Hu and Du, Daniel K and Yuan, Zehuan and Wu, Xinglong},
        journal={arXiv preprint arXiv:2412.03069},
        year={2024}
      }
    </code></pre>
    </div>
</section>
<!--End BibTex citation -->

<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=2sfPs30m5ae1yTKBwO42x_ZaAXQ96jZjosVxeZpvnA4&cl=ffffff&w=a"></script>

  
  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
